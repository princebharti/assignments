{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective: \n",
    "To classify a review(amazon fine food reviews) into negative or positive class using Naive Bayes algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split,TimeSeriesSplit,RandomizedSearchCV\n",
    "from sklearn.utils.class_weight import compute_class_weight,compute_sample_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_distances,euclidean_distances,cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading cleaned amazon review csv\n",
    "df=pd.read_csv('cleaned_amazon_reviews.csv')\n",
    "df=df.dropna()\n",
    "df_sample=df #here we can modify our sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample=df_sample.sort_values(by=['Time'],axis=0,ascending=True)  # sorting of dataframe for Time Based splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting reviews to vector\n",
    "list_of_sentence=[] #list of sentence to be used for countvectorizer\n",
    "for sentence in df_sample['CleanText'].values:\n",
    "    li=sentence\n",
    "    list_of_sentence.append(li)\n",
    "\n",
    "tfidf_model=TfidfVectorizer(ngram_range=(1, 2)) #using count-BOW and 1 & 2 grams \n",
    "tfidf_bow_review_matrix=tfidf_model.fit_transform(list_of_sentence) #training our model and converting text to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization of feature matrix\n",
    "X=StandardScaler(with_mean=False).fit_transform(tfidf_bow_review_matrix)\n",
    "# In LR we know for negative class we take -1 and for positive class +1.\n",
    "y=y=df_sample['class'].apply(lambda string: 1 if string=='positive' else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,shuffle=False) # 70-30 split without shuffling(used for TBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Grid search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) using L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=<generator object TimeSeriesSplit.split at 0x3ffe484ceb48>,\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.0001, 1e-05, 1e-06, 1e-07, 1e-08, 1e-09, 1e-10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model=LogisticRegression(random_state=0,n_jobs=-1)\n",
    "tscv=TimeSeriesSplit(n_splits=10) # timeseries split for using timeseries based cross-validation\n",
    "c=[0.0001,0.00001,0.000001,0.0000001,0.00000001,0.000000001,0.0000000001] #list of c to passed to the LR model\n",
    "y_train_sample_weight=compute_sample_weight(class_weight='balanced',y=y_train)\n",
    "gs=GridSearchCV(lr_model,param_grid={'C':c},scoring='accuracy',cv=tscv.split(X_train))\n",
    "gs.fit(X_train,y_train,sample_weight=y_train_sample_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88938416829172595"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict=gs.predict(X_test)\n",
    "y_test_sample_weight=compute_sample_weight(class_weight='balanced',y=y_test)\n",
    "accuracy_score(y_test,y_predict,sample_weight=y_test_sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1e-07}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) using L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=<generator object TimeSeriesSplit.split at 0x3fff0083c570>,\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [100, 10, 1, 0.1, 0.01, 0.001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model=LogisticRegression(penalty='l1',random_state=0,n_jobs=-1)\n",
    "tscv=TimeSeriesSplit(n_splits=10) # timeseries split for using timeseries based cross-validation\n",
    "c=[100,10,1,0.1,0.01,0.001] #list of c to passed to the LR model\n",
    "y_train_sample_weight=compute_sample_weight(class_weight='balanced',y=y_train)\n",
    "gs=GridSearchCV(lr_model,param_grid={'C':c},scoring='accuracy',cv=tscv.split(X_train))\n",
    "gs.fit(X_train,y_train,sample_weight=y_train_sample_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8491609411664528"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict=gs.predict(X_test)\n",
    "y_test_sample_weight=compute_sample_weight(class_weight='balanced',y=y_test)\n",
    "accuracy_score(y_test,y_predict,sample_weight=y_test_sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Random Search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) using L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=<generator object TimeSeriesSplit.split at 0x3fff0b876728>,\n",
       "          error_score='raise',\n",
       "          estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'C': array([  4.96788e-07,   8.81614e-07,   2.06406e-07,   6.43269e-07,\n",
       "         8.39432e-07,   5.33538e-07,   4.41028e-07,   2.38218e-08,\n",
       "         4.05763e-07,   7.12149e-07,   2.13623e-07,   6.39345e-07,\n",
       "         6.08885e-07,   3.15056e-07,   8.01074e-07,   7.61407e-07,\n",
       "         1.15046e-07,   1.39763e-07,   3.42908e-07,   5.00267e-07])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model=LogisticRegression(random_state=0,n_jobs=-1)\n",
    "tscv=TimeSeriesSplit(n_splits=10) # timeseries split for using timeseries based cross-validation\n",
    "param_grid=np.random.uniform(0.00000001,0.000001,20)\n",
    "y_train_sample_weight=compute_sample_weight(class_weight='balanced',y=y_train)\n",
    "rs=RandomizedSearchCV(lr_model,param_distributions={'C':param_grid},scoring='accuracy',cv=tscv.split(X_train))\n",
    "rs.fit(X_train,y_train,sample_weight=y_train_sample_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88284830761180488"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict=rs.predict(X_test)\n",
    "y_test_sample_weight=compute_sample_weight(class_weight='balanced',y=y_test)\n",
    "accuracy_score(y_test,y_predict,sample_weight=y_test_sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.3976274849389749e-07}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) using L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=<generator object TimeSeriesSplit.split at 0x3ffef6264fc0>,\n",
       "          error_score='raise',\n",
       "          estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'C': array([ 0.88585,  0.17497,  0.93794,  0.49264,  0.2191 ,  0.14032,\n",
       "        0.24433,  0.99066,  0.45169,  0.69966,  0.79476,  0.2439 ,\n",
       "        0.96409,  0.09913,  0.49571,  0.94768,  0.30067,  0.34672,\n",
       "        0.45416,  0.32356])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model=LogisticRegression(penalty='l1',random_state=0,n_jobs=-1)\n",
    "tscv=TimeSeriesSplit(n_splits=10) # timeseries split for using timeseries based cross-validation\n",
    "param_grid=np.random.uniform(0.001,1,20)\n",
    "y_train_sample_weight=compute_sample_weight(class_weight='balanced',y=y_train)\n",
    "rs=RandomizedSearchCV(lr_model,param_distributions={'C':param_grid},scoring='accuracy',cv=tscv.split(X_train))\n",
    "rs.fit(X_train,y_train,sample_weight=y_train_sample_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82299552894908301"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict=rs.predict(X_test)\n",
    "y_test_sample_weight=compute_sample_weight(class_weight='balanced',y=y_test)\n",
    "accuracy_score(y_test,y_predict,sample_weight=y_test_sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.94768333445684139}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase in sparsity(decrease in non zero element) with increase in Lambda(decrese in C parameter) in L1 regularizatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=  0.01  no of non-zero in weight vector 773201 error=  0.204399925328\n",
      "lambda=  0.1  no of non-zero in weight vector 458474 error=  0.218262914994\n",
      "lambda=  1.0  no of non-zero in weight vector 200697 error=  0.177229013404\n",
      "lambda=  10.0  no of non-zero in weight vector 158700 error=  0.170447921502\n",
      "lambda=  100.0  no of non-zero in weight vector 125392 error=  0.150839058834\n",
      "lambda=  1000.0  no of non-zero in weight vector 21333 error=  0.111753077968\n",
      "lambda=  10000.0  no of non-zero in weight vector 34 error=  0.248966762935\n"
     ]
    }
   ],
   "source": [
    "c_list=[100,10,1,0.1,0.01,0.001,0.0001]\n",
    "y_train_sample_weight=compute_sample_weight(class_weight='balanced',y=y_train)\n",
    "y_test_sample_weight=compute_sample_weight(class_weight='balanced',y=y_test)\n",
    "for c in c_list:\n",
    "    lr_model=LogisticRegression(penalty='l1',C=c,random_state=0,n_jobs=-1)\n",
    "    lr_model.fit(X_train,y_train,y_train_sample_weight)\n",
    "    y_predict=lr_model.predict(X_test)\n",
    "    error=1-accuracy_score(y_test,y_predict,sample_weight=y_test_sample_weight)\n",
    "    print('lambda= ',1/c,' no of non-zero in weight vector',np.count_nonzero(lr_model.coef_) , 'error= ',error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature collinearity test-perbutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('cleaned_amazon_reviews.csv')\n",
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for balanced dataset where n=#for each class\n",
    "n=2500\n",
    "df_positive=df[df['class']=='positive'].sample(n)\n",
    "df_negative=df[df['class']=='negative'].sample(n)\n",
    "df_sample=pd.concat([df_positive,df_negative])\n",
    "df_sample=df_sample.sort_values(by=['Time'],axis=0,ascending=True)  #dataframe being sorted based by time \n",
    "\n",
    "list_of_sentence=[] # list of sentences \n",
    "for sentence in df_sample['CleanText'].values:\n",
    "    list_of_sentence.append(sentence)\n",
    "tfidf_model=TfidfVectorizer(ngram_range=(1,2))    \n",
    "tfidf_review_matrix=tfidf_model.fit_transform(list_of_sentence) #training our tf-idf model\n",
    "\n",
    "X=StandardScaler(with_mean=False).fit_transform(tfidf_review_matrix)\n",
    "# In LR we know for negative class we take -1 and for positive class +1.\n",
    "y=y=df_sample['class'].apply(lambda string: 1 if string=='positive' else -1)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,shuffle=False) # 70-30 split without shuffling(used for TBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=<generator object TimeSeriesSplit.split at 0x3ffefeea8308>,\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.0001, 1e-05, 1e-06, 1e-07, 1e-08, 1e-09, 1e-10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model=LogisticRegression(random_state=0,n_jobs=-1)\n",
    "tscv=TimeSeriesSplit(n_splits=10) # timeseries split for using timeseries based cross-validation\n",
    "c=[0.0001,0.00001,0.000001,0.0000001,0.00000001,0.000000001,0.0000000001] #list of c to passed to the LR model\n",
    "y_train_sample_weight=compute_sample_weight(class_weight='balanced',y=y_train)\n",
    "gs=GridSearchCV(lr_model,param_grid={'C':c},scoring='accuracy',cv=tscv.split(X_train))\n",
    "gs.fit(X_train,y_train,sample_weight=y_train_sample_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83675213675213689"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict=gs.predict(X_test)\n",
    "y_test_sample_weight=compute_sample_weight(class_weight='balanced',y=y_test)\n",
    "accuracy_score(y_test,y_predict,sample_weight=y_test_sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1e-07}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=gs.best_estimator_.coef_ # storing w as w1 before adding noise to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_noise=X_train+csr_matrix(np.random.normal(0,0.10,X_train.shape)) # addding noise to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=<generator object TimeSeriesSplit.split at 0x3fff01cb6eb8>,\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.0001, 1e-05, 1e-06, 1e-07, 1e-08, 1e-09, 1e-10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model=LogisticRegression(random_state=0,n_jobs=-1)\n",
    "tscv=TimeSeriesSplit(n_splits=10) # timeseries split for using timeseries based cross-validation\n",
    "c=[0.0001,0.00001,0.000001,0.0000001,0.00000001,0.000000001,0.0000000001] #list of c to passed to the LR model\n",
    "y_train_sample_weight=compute_sample_weight(class_weight='balanced',y=y_train)\n",
    "gs=GridSearchCV(lr_model,param_grid={'C':c},scoring='accuracy',cv=tscv.split(X_train))\n",
    "gs.fit(X_noise,y_train,sample_weight=y_train_sample_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83643162393162407"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict=gs.predict(X_test)\n",
    "y_test_sample_weight=compute_sample_weight(class_weight='balanced',y=y_test)\n",
    "accuracy_score(y_test,y_predict,sample_weight=y_test_sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1e-07}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2=gs.best_estimator_.coef_ # storing w as w2 before adding noise to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist=DistanceMetric.get_metric('euclidean').pairwise(w1,w2) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00011359]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99538119]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(w1,w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result:\n",
    "    As we can see  that cosine_similarity(w1,w2)=0.99538119 which is approx equal to 1 we can say that mullticollineariry is not present in the data and if it is there it is very very small and hence we can use weight vector as feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Imporantance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading cleaned amazon review csv\n",
    "df=pd.read_csv('cleaned_amazon_reviews.csv')\n",
    "df=df.dropna()\n",
    "df_sample=df #here we can modify our sample size\n",
    "df_sample=df_sample.sort_values(by=['Time'],axis=0,ascending=True)  # sorting of dataframe for Time Based splitting\n",
    "# converting reviews to vector\n",
    "list_of_sentence=[] #list of sentence to be used for countvectorizer\n",
    "for sentence in df_sample['CleanText'].values:\n",
    "    li=sentence\n",
    "    list_of_sentence.append(li)\n",
    "\n",
    "tfidf_model=TfidfVectorizer(ngram_range=(1, 2)) #using count-BOW and 1 & 2 grams \n",
    "tfidf_bow_review_matrix=tfidf_model.fit_transform(list_of_sentence) #training our model and converting text to vector\n",
    "\n",
    "# standardization of feature matrix\n",
    "X=StandardScaler(with_mean=False).fit_transform(tfidf_bow_review_matrix)\n",
    "# In LR we know for negative class we take -1 and for positive class +1.\n",
    "y=y=df_sample['class'].apply(lambda string: 1 if string=='positive' else -1)\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,shuffle=False) # 70-30 split without shuffling(used for TBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230100, 3178748)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100 #top n features \n",
    "y_train_sample_weight=compute_sample_weight(class_weight='balanced',y=y_train)\n",
    "lr_model=LogisticRegression(C=1e-07,random_state=0,n_jobs=-1) # training the model for Weight vector\n",
    "lr_model.fit(X_train,y_train,sample_weight=y_train_sample_weight)\n",
    "\n",
    "di_positive={}#dict for storing key as +ve w components  and value as index corrosponding to that component\n",
    "di_negative={}#dict for storing key as -ve w components  and value as index corrosponding to that component\n",
    "\n",
    "li=lr_model.coef_[0,:]\n",
    "feature_name=tfidf_model.get_feature_names() # features name \n",
    "for value,key in enumerate(li):\n",
    "    if key>0:\n",
    "        di_positive[key]=value\n",
    "    else:\n",
    "        di_negative[abs(key)]=value\n",
    "top_positive_features=[]\n",
    "top_negative_features=[]\n",
    "for key in sorted(di_positive,reverse=True)[:n]: #sorted the dict in descending order to get  n imp +ve weights components\n",
    "    index=di_positive[key] #fetching the\n",
    "    top_positive_features.append(feature_name[index])\n",
    "    \n",
    "for key in sorted(di_negative,reverse=True)[:n]: #sorted the dict in descending order to get  n imp -ve weights components\n",
    "    index=di_negative[key] #fetching the\n",
    "    top_negative_features.append(feature_name[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great',\n",
       " 'love',\n",
       " 'best',\n",
       " 'delici',\n",
       " 'good',\n",
       " 'find',\n",
       " 'perfect',\n",
       " 'favorit',\n",
       " 'make',\n",
       " 'use',\n",
       " 'easi',\n",
       " 'high recommend',\n",
       " 'excel',\n",
       " 'nice',\n",
       " 'keep',\n",
       " 'enjoy',\n",
       " 'wonder',\n",
       " 'this',\n",
       " 'snack',\n",
       " 'year',\n",
       " 'store',\n",
       " 'littl',\n",
       " 'alway',\n",
       " 'add',\n",
       " 'high',\n",
       " 'tasti',\n",
       " 'thank',\n",
       " 'quick',\n",
       " 'recommend',\n",
       " 'also',\n",
       " 'well',\n",
       " 'price',\n",
       " 'without',\n",
       " 'carri',\n",
       " 'happi',\n",
       " 'day',\n",
       " 'everi',\n",
       " 'it',\n",
       " 'tast great',\n",
       " 'found',\n",
       " 'these',\n",
       " 'smooth',\n",
       " 'ive',\n",
       " 'amaz',\n",
       " 'fresh',\n",
       " 'healthi',\n",
       " 'morn',\n",
       " 'famili',\n",
       " 'fast',\n",
       " 'tea',\n",
       " 'glad',\n",
       " 'satisfi',\n",
       " 'treat',\n",
       " 'friend',\n",
       " 'this best',\n",
       " 'need',\n",
       " 'right',\n",
       " 'this great',\n",
       " 'addict',\n",
       " 'bit',\n",
       " 'time',\n",
       " 'rich',\n",
       " 'great tast',\n",
       " 'mix',\n",
       " 'hard find',\n",
       " 'long',\n",
       " 'great product',\n",
       " 'cook',\n",
       " 'yummi',\n",
       " 'definit',\n",
       " 'flavor',\n",
       " 'free',\n",
       " 'awesom',\n",
       " 'husband',\n",
       " 'avail',\n",
       " 'fantast',\n",
       " 'work',\n",
       " 'beat',\n",
       " 'breakfast',\n",
       " 'home',\n",
       " 'great price',\n",
       " 'meal',\n",
       " 'hook',\n",
       " 'sweet',\n",
       " 'they',\n",
       " 'kid',\n",
       " 'help',\n",
       " 'one best',\n",
       " 'local',\n",
       " 'make great',\n",
       " 'calori',\n",
       " 'cold',\n",
       " 'especi',\n",
       " 'abl',\n",
       " 'realli good',\n",
       " 'conveni',\n",
       " 'sometim',\n",
       " 'everyon',\n",
       " 'ice',\n",
       " 'delight']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_positive_features # positive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disappoint',\n",
       " 'not',\n",
       " 'return',\n",
       " 'wast',\n",
       " 'wast money',\n",
       " 'worst',\n",
       " 'not buy',\n",
       " 'aw',\n",
       " 'horribl',\n",
       " 'would not',\n",
       " 'terribl',\n",
       " 'bad',\n",
       " 'money',\n",
       " 'threw',\n",
       " 'not recommend',\n",
       " 'not worth',\n",
       " 'refund',\n",
       " 'stale',\n",
       " 'disgust',\n",
       " 'veri disappoint',\n",
       " 'wont buy',\n",
       " 'not good',\n",
       " 'dont wast',\n",
       " 'thought',\n",
       " 'descript',\n",
       " 'not order',\n",
       " 'not purchas',\n",
       " 'tast like',\n",
       " 'away',\n",
       " 'unfortun',\n",
       " 'wors',\n",
       " 'poor',\n",
       " 'mayb',\n",
       " 'pictur',\n",
       " 'never buy',\n",
       " 'receiv',\n",
       " 'didnt',\n",
       " 'nasti',\n",
       " 'bewar',\n",
       " 'gross',\n",
       " 'not even',\n",
       " 'throw',\n",
       " 'expir',\n",
       " 'bland',\n",
       " 'yuck',\n",
       " 'tasteless',\n",
       " 'trash',\n",
       " 'rip',\n",
       " 'contact',\n",
       " 'stuck',\n",
       " 'mistak',\n",
       " 'threw away',\n",
       " 'would',\n",
       " 'sorri',\n",
       " 'review',\n",
       " 'noth like',\n",
       " 'weak',\n",
       " 'never order',\n",
       " 'ined',\n",
       " 'two star',\n",
       " 'throw away',\n",
       " 'bad batch',\n",
       " 'tast aw',\n",
       " 'end throw',\n",
       " 'buyer bewar',\n",
       " 'mislead',\n",
       " 'worst tast',\n",
       " 'tast bad',\n",
       " 'hope',\n",
       " 'unpleas',\n",
       " 'cancel',\n",
       " 'item',\n",
       " 'list',\n",
       " 'label',\n",
       " 'disappoint product',\n",
       " 'china',\n",
       " 'product not',\n",
       " 'tast horribl',\n",
       " 'not tast',\n",
       " 'will not',\n",
       " 'want like',\n",
       " 'definit not',\n",
       " 'tast noth',\n",
       " 'tast terribl',\n",
       " 'realli disappoint',\n",
       " 'this not',\n",
       " 'read',\n",
       " 'open',\n",
       " 'noth',\n",
       " 'extrem disappoint',\n",
       " 'guess',\n",
       " 'wouldnt',\n",
       " 'date',\n",
       " 'got bad',\n",
       " 'high hope',\n",
       " 'not return',\n",
       " 'buyer',\n",
       " 'box',\n",
       " 'dont buy',\n",
       " 'this worst']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_negative_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
