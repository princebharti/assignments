
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Logistic Regression}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{objective}{%
\subsubsection{Objective:}\label{objective}}

To classify a review(amazon fine food reviews) into negative or positive
class using Naive Bayes algorithm.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{feature\PYZus{}extraction}\PY{n+nn}{.}\PY{n+nn}{text} \PY{k}{import} \PY{n}{TfidfVectorizer}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{StandardScaler}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{LogisticRegression}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{GridSearchCV}\PY{p}{,}\PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{,}\PY{n}{TimeSeriesSplit}\PY{p}{,}\PY{n}{RandomizedSearchCV}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{utils}\PY{n+nn}{.}\PY{n+nn}{class\PYZus{}weight} \PY{k}{import} \PY{n}{compute\PYZus{}class\PYZus{}weight}\PY{p}{,}\PY{n}{compute\PYZus{}sample\PYZus{}weight}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{accuracy\PYZus{}score}
         \PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{sparse} \PY{k}{import} \PY{n}{csr\PYZus{}matrix}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics}\PY{n+nn}{.}\PY{n+nn}{pairwise} \PY{k}{import} \PY{n}{cosine\PYZus{}distances}\PY{p}{,}\PY{n}{euclidean\PYZus{}distances}\PY{p}{,}\PY{n}{cosine\PYZus{}similarity}
         \PY{k+kn}{import} \PY{n+nn}{warnings}
         \PY{n}{warnings}\PY{o}{.}\PY{n}{filterwarnings}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ignore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c+c1}{\PYZsh{}reading cleaned amazon review csv}
        \PY{n}{df}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cleaned\PYZus{}amazon\PYZus{}reviews.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{df}\PY{o}{=}\PY{n}{df}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}
        \PY{n}{df\PYZus{}sample}\PY{o}{=}\PY{n}{df} \PY{c+c1}{\PYZsh{}here we can modify our sample size}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{df\PYZus{}sample}\PY{o}{=}\PY{n}{df\PYZus{}sample}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{by}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{ascending}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}  \PY{c+c1}{\PYZsh{} sorting of dataframe for Time Based splitting}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{c+c1}{\PYZsh{} converting reviews to vector}
        \PY{n}{list\PYZus{}of\PYZus{}sentence}\PY{o}{=}\PY{p}{[}\PY{p}{]} \PY{c+c1}{\PYZsh{}list of sentence to be used for countvectorizer}
        \PY{k}{for} \PY{n}{sentence} \PY{o+ow}{in} \PY{n}{df\PYZus{}sample}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CleanText}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{:}
            \PY{n}{li}\PY{o}{=}\PY{n}{sentence}
            \PY{n}{list\PYZus{}of\PYZus{}sentence}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{li}\PY{p}{)}
        
        \PY{n}{tfidf\PYZus{}model}\PY{o}{=}\PY{n}{TfidfVectorizer}\PY{p}{(}\PY{n}{ngram\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}using count\PYZhy{}BOW and 1 \PYZam{} 2 grams }
        \PY{n}{tfidf\PYZus{}bow\PYZus{}review\PYZus{}matrix}\PY{o}{=}\PY{n}{tfidf\PYZus{}model}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{list\PYZus{}of\PYZus{}sentence}\PY{p}{)} \PY{c+c1}{\PYZsh{}training our model and converting text to vector}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{c+c1}{\PYZsh{} standardization of feature matrix}
        \PY{n}{X}\PY{o}{=}\PY{n}{StandardScaler}\PY{p}{(}\PY{n}{with\PYZus{}mean}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{tfidf\PYZus{}bow\PYZus{}review\PYZus{}matrix}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} In LR we know for negative class we take \PYZhy{}1 and for positive class +1.}
        \PY{n}{y}\PY{o}{=}\PY{n}{y}\PY{o}{=}\PY{n}{df\PYZus{}sample}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{string}\PY{p}{:} \PY{l+m+mi}{1} \PY{k}{if} \PY{n}{string}\PY{o}{==}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{positive}\PY{l+s+s1}{\PYZsq{}} \PY{k}{else} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}test}\PY{o}{=}\PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{,}\PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)} \PY{c+c1}{\PYZsh{} 70\PYZhy{}30 split without shuffling(used for TBS)}
\end{Verbatim}


    \hypertarget{using-grid-search}{%
\subsubsection{Using Grid search}\label{using-grid-search}}

    \hypertarget{using-l2-regularization}{%
\paragraph{1) using L2 Regularization}\label{using-l2-regularization}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{n}{lr\PYZus{}model}\PY{o}{=}\PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{tscv}\PY{o}{=}\PY{n}{TimeSeriesSplit}\PY{p}{(}\PY{n}{n\PYZus{}splits}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)} \PY{c+c1}{\PYZsh{} timeseries split for using timeseries based cross\PYZhy{}validation}
         \PY{n}{c}\PY{o}{=}\PY{p}{[}\PY{l+m+mf}{0.0001}\PY{p}{,}\PY{l+m+mf}{0.00001}\PY{p}{,}\PY{l+m+mf}{0.000001}\PY{p}{,}\PY{l+m+mf}{0.0000001}\PY{p}{,}\PY{l+m+mf}{0.00000001}\PY{p}{,}\PY{l+m+mf}{0.000000001}\PY{p}{,}\PY{l+m+mf}{0.0000000001}\PY{p}{]} \PY{c+c1}{\PYZsh{}list of c to passed to the LR model}
         \PY{n}{y\PYZus{}train\PYZus{}sample\PYZus{}weight}\PY{o}{=}\PY{n}{compute\PYZus{}sample\PYZus{}weight}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{balanced}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{y}\PY{o}{=}\PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{gs}\PY{o}{=}\PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{lr\PYZus{}model}\PY{p}{,}\PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{c}\PY{p}{\PYZcb{}}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{n}{tscv}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{)}
         \PY{n}{gs}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{sample\PYZus{}weight}\PY{o}{=}\PY{n}{y\PYZus{}train\PYZus{}sample\PYZus{}weight}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}42}]:} GridSearchCV(cv=<generator object TimeSeriesSplit.split at 0x3ffe484ceb48>,
                error\_score='raise',
                estimator=LogisticRegression(C=1.0, class\_weight=None, dual=False, fit\_intercept=True,
                   intercept\_scaling=1, max\_iter=100, multi\_class='ovr', n\_jobs=-1,
                   penalty='l2', random\_state=0, solver='liblinear', tol=0.0001,
                   verbose=0, warm\_start=False),
                fit\_params=None, iid=True, n\_jobs=1,
                param\_grid=\{'C': [0.0001, 1e-05, 1e-06, 1e-07, 1e-08, 1e-09, 1e-10]\},
                pre\_dispatch='2*n\_jobs', refit=True, return\_train\_score=True,
                scoring='accuracy', verbose=0)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{n}{y\PYZus{}predict}\PY{o}{=}\PY{n}{gs}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
         \PY{n}{y\PYZus{}test\PYZus{}sample\PYZus{}weight}\PY{o}{=}\PY{n}{compute\PYZus{}sample\PYZus{}weight}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{balanced}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{y}\PY{o}{=}\PY{n}{y\PYZus{}test}\PY{p}{)}
         \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}predict}\PY{p}{,}\PY{n}{sample\PYZus{}weight}\PY{o}{=}\PY{n}{y\PYZus{}test\PYZus{}sample\PYZus{}weight}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}29}]:} 0.88938416829172595
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{n}{gs}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}31}]:} \{'C': 1e-07\}
\end{Verbatim}
            
    \hypertarget{using-l1-regularization}{%
\paragraph{2) using L1 Regularization}\label{using-l1-regularization}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}129}]:} \PY{n}{lr\PYZus{}model}\PY{o}{=}\PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{penalty}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{tscv}\PY{o}{=}\PY{n}{TimeSeriesSplit}\PY{p}{(}\PY{n}{n\PYZus{}splits}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)} \PY{c+c1}{\PYZsh{} timeseries split for using timeseries based cross\PYZhy{}validation}
          \PY{n}{c}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{100}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.01}\PY{p}{,}\PY{l+m+mf}{0.001}\PY{p}{]} \PY{c+c1}{\PYZsh{}list of c to passed to the LR model}
          \PY{n}{y\PYZus{}train\PYZus{}sample\PYZus{}weight}\PY{o}{=}\PY{n}{compute\PYZus{}sample\PYZus{}weight}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{balanced}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{y}\PY{o}{=}\PY{n}{y\PYZus{}train}\PY{p}{)}
          \PY{n}{gs}\PY{o}{=}\PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{lr\PYZus{}model}\PY{p}{,}\PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{c}\PY{p}{\PYZcb{}}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{n}{tscv}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{)}
          \PY{n}{gs}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{sample\PYZus{}weight}\PY{o}{=}\PY{n}{y\PYZus{}train\PYZus{}sample\PYZus{}weight}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}129}]:} GridSearchCV(cv=<generator object TimeSeriesSplit.split at 0x3fff0083c570>,
                 error\_score='raise',
                 estimator=LogisticRegression(C=1.0, class\_weight=None, dual=False, fit\_intercept=True,
                    intercept\_scaling=1, max\_iter=100, multi\_class='ovr', n\_jobs=-1,
                    penalty='l1', random\_state=0, solver='liblinear', tol=0.0001,
                    verbose=0, warm\_start=False),
                 fit\_params=None, iid=True, n\_jobs=1,
                 param\_grid=\{'C': [100, 10, 1, 0.1, 0.01, 0.001]\},
                 pre\_dispatch='2*n\_jobs', refit=True, return\_train\_score=True,
                 scoring='accuracy', verbose=0)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}130}]:} \PY{n}{y\PYZus{}predict}\PY{o}{=}\PY{n}{gs}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
          \PY{n}{y\PYZus{}test\PYZus{}sample\PYZus{}weight}\PY{o}{=}\PY{n}{compute\PYZus{}sample\PYZus{}weight}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{balanced}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{y}\PY{o}{=}\PY{n}{y\PYZus{}test}\PY{p}{)}
          \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}predict}\PY{p}{,}\PY{n}{sample\PYZus{}weight}\PY{o}{=}\PY{n}{y\PYZus{}test\PYZus{}sample\PYZus{}weight}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}130}]:} 0.8491609411664528
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}131}]:} \PY{n}{gs}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}131}]:} \{'C': 0.01\}
\end{Verbatim}
            
    \hypertarget{using-random-search}{%
\subsubsection{Using Random Search}\label{using-random-search}}

    \hypertarget{using-l2-regularization}{%
\paragraph{1) using L2 Regularization}\label{using-l2-regularization}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}132}]:} \PY{n}{lr\PYZus{}model}\PY{o}{=}\PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{tscv}\PY{o}{=}\PY{n}{TimeSeriesSplit}\PY{p}{(}\PY{n}{n\PYZus{}splits}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)} \PY{c+c1}{\PYZsh{} timeseries split for using timeseries based cross\PYZhy{}validation}
          \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{uniform}\PY{p}{(}\PY{l+m+mf}{0.00000001}\PY{p}{,}\PY{l+m+mf}{0.000001}\PY{p}{,}\PY{l+m+mi}{20}\PY{p}{)}
          \PY{n}{y\PYZus{}train\PYZus{}sample\PYZus{}weight}\PY{o}{=}\PY{n}{compute\PYZus{}sample\PYZus{}weight}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{balanced}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{y}\PY{o}{=}\PY{n}{y\PYZus{}train}\PY{p}{)}
          \PY{n}{rs}\PY{o}{=}\PY{n}{RandomizedSearchCV}\PY{p}{(}\PY{n}{lr\PYZus{}model}\PY{p}{,}\PY{n}{param\PYZus{}distributions}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{param\PYZus{}grid}\PY{p}{\PYZcb{}}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{n}{tscv}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{)}
          \PY{n}{rs}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{sample\PYZus{}weight}\PY{o}{=}\PY{n}{y\PYZus{}train\PYZus{}sample\PYZus{}weight}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}132}]:} RandomizedSearchCV(cv=<generator object TimeSeriesSplit.split at 0x3fff0b876728>,
                    error\_score='raise',
                    estimator=LogisticRegression(C=1.0, class\_weight=None, dual=False, fit\_intercept=True,
                    intercept\_scaling=1, max\_iter=100, multi\_class='ovr', n\_jobs=-1,
                    penalty='l2', random\_state=0, solver='liblinear', tol=0.0001,
                    verbose=0, warm\_start=False),
                    fit\_params=None, iid=True, n\_iter=10, n\_jobs=1,
                    param\_distributions=\{'C': array([  4.96788e-07,   8.81614e-07,   2.06406e-07,   6.43269e-07,
                   8.39432e-07,   5.33538e-07,   4.41028e-07,   2.38218e-08,
                   4.05763e-07,   7.12149e-07,   2.13623e-07,   6.39345e-07,
                   6.08885e-07,   3.15056e-07,   8.01074e-07,   7.61407e-07,
                   1.15046e-07,   1.39763e-07,   3.42908e-07,   5.00267e-07])\},
                    pre\_dispatch='2*n\_jobs', random\_state=None, refit=True,
                    return\_train\_score=True, scoring='accuracy', verbose=0)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}133}]:} \PY{n}{y\PYZus{}predict}\PY{o}{=}\PY{n}{rs}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
          \PY{n}{y\PYZus{}test\PYZus{}sample\PYZus{}weight}\PY{o}{=}\PY{n}{compute\PYZus{}sample\PYZus{}weight}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{balanced}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{y}\PY{o}{=}\PY{n}{y\PYZus{}test}\PY{p}{)}
          \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}predict}\PY{p}{,}\PY{n}{sample\PYZus{}weight}\PY{o}{=}\PY{n}{y\PYZus{}test\PYZus{}sample\PYZus{}weight}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}133}]:} 0.88284830761180488
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}134}]:} \PY{n}{rs}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}134}]:} \{'C': 1.3976274849389749e-07\}
\end{Verbatim}
            
    \hypertarget{using-l1-regularization}{%
\paragraph{2) using L1 Regularization}\label{using-l1-regularization}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}144}]:} \PY{n}{lr\PYZus{}model}\PY{o}{=}\PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{penalty}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{tscv}\PY{o}{=}\PY{n}{TimeSeriesSplit}\PY{p}{(}\PY{n}{n\PYZus{}splits}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)} \PY{c+c1}{\PYZsh{} timeseries split for using timeseries based cross\PYZhy{}validation}
          \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{uniform}\PY{p}{(}\PY{l+m+mf}{0.001}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{20}\PY{p}{)}
          \PY{n}{y\PYZus{}train\PYZus{}sample\PYZus{}weight}\PY{o}{=}\PY{n}{compute\PYZus{}sample\PYZus{}weight}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{balanced}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{y}\PY{o}{=}\PY{n}{y\PYZus{}train}\PY{p}{)}
          \PY{n}{rs}\PY{o}{=}\PY{n}{RandomizedSearchCV}\PY{p}{(}\PY{n}{lr\PYZus{}model}\PY{p}{,}\PY{n}{param\PYZus{}distributions}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{param\PYZus{}grid}\PY{p}{\PYZcb{}}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{n}{tscv}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{)}
          \PY{n}{rs}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{sample\PYZus{}weight}\PY{o}{=}\PY{n}{y\PYZus{}train\PYZus{}sample\PYZus{}weight}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}144}]:} RandomizedSearchCV(cv=<generator object TimeSeriesSplit.split at 0x3ffef6264fc0>,
                    error\_score='raise',
                    estimator=LogisticRegression(C=1.0, class\_weight=None, dual=False, fit\_intercept=True,
                    intercept\_scaling=1, max\_iter=100, multi\_class='ovr', n\_jobs=-1,
                    penalty='l1', random\_state=0, solver='liblinear', tol=0.0001,
                    verbose=0, warm\_start=False),
                    fit\_params=None, iid=True, n\_iter=10, n\_jobs=1,
                    param\_distributions=\{'C': array([ 0.88585,  0.17497,  0.93794,  0.49264,  0.2191 ,  0.14032,
                  0.24433,  0.99066,  0.45169,  0.69966,  0.79476,  0.2439 ,
                  0.96409,  0.09913,  0.49571,  0.94768,  0.30067,  0.34672,
                  0.45416,  0.32356])\},
                    pre\_dispatch='2*n\_jobs', random\_state=None, refit=True,
                    return\_train\_score=True, scoring='accuracy', verbose=0)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}145}]:} \PY{n}{y\PYZus{}predict}\PY{o}{=}\PY{n}{rs}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
          \PY{n}{y\PYZus{}test\PYZus{}sample\PYZus{}weight}\PY{o}{=}\PY{n}{compute\PYZus{}sample\PYZus{}weight}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{balanced}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{y}\PY{o}{=}\PY{n}{y\PYZus{}test}\PY{p}{)}
          \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}predict}\PY{p}{,}\PY{n}{sample\PYZus{}weight}\PY{o}{=}\PY{n}{y\PYZus{}test\PYZus{}sample\PYZus{}weight}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}145}]:} 0.82299552894908301
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}146}]:} \PY{n}{rs}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}146}]:} \{'C': 0.94768333445684139\}
\end{Verbatim}
            
    \hypertarget{increase-in-sparsitydecrease-in-non-zero-element-with-increase-in-lambdadecrese-in-c-parameter-in-l1-regularizatation}{%
\subsubsection{Increase in sparsity(decrease in non zero element) with
increase in Lambda(decrese in C parameter) in L1
regularizatation}\label{increase-in-sparsitydecrease-in-non-zero-element-with-increase-in-lambdadecrese-in-c-parameter-in-l1-regularizatation}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}143}]:} \PY{n}{c\PYZus{}list}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{100}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.01}\PY{p}{,}\PY{l+m+mf}{0.001}\PY{p}{,}\PY{l+m+mf}{0.0001}\PY{p}{]}
          \PY{n}{y\PYZus{}train\PYZus{}sample\PYZus{}weight}\PY{o}{=}\PY{n}{compute\PYZus{}sample\PYZus{}weight}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{balanced}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{y}\PY{o}{=}\PY{n}{y\PYZus{}train}\PY{p}{)}
          \PY{n}{y\PYZus{}test\PYZus{}sample\PYZus{}weight}\PY{o}{=}\PY{n}{compute\PYZus{}sample\PYZus{}weight}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{balanced}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{y}\PY{o}{=}\PY{n}{y\PYZus{}test}\PY{p}{)}
          \PY{k}{for} \PY{n}{c} \PY{o+ow}{in} \PY{n}{c\PYZus{}list}\PY{p}{:}
              \PY{n}{lr\PYZus{}model}\PY{o}{=}\PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{penalty}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{C}\PY{o}{=}\PY{n}{c}\PY{p}{,}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
              \PY{n}{lr\PYZus{}model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train\PYZus{}sample\PYZus{}weight}\PY{p}{)}
              \PY{n}{y\PYZus{}predict}\PY{o}{=}\PY{n}{lr\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
              \PY{n}{error}\PY{o}{=}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}predict}\PY{p}{,}\PY{n}{sample\PYZus{}weight}\PY{o}{=}\PY{n}{y\PYZus{}test\PYZus{}sample\PYZus{}weight}\PY{p}{)}
              \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lambda= }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+m+mi}{1}\PY{o}{/}\PY{n}{c}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ no of non\PYZhy{}zero in weight vector}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{count\PYZus{}nonzero}\PY{p}{(}\PY{n}{lr\PYZus{}model}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{)} \PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{error= }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{error}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
lambda=  0.01  no of non-zero in weight vector 773201 error=  0.204399925328
lambda=  0.1  no of non-zero in weight vector 458474 error=  0.218262914994
lambda=  1.0  no of non-zero in weight vector 200697 error=  0.177229013404
lambda=  10.0  no of non-zero in weight vector 158700 error=  0.170447921502
lambda=  100.0  no of non-zero in weight vector 125392 error=  0.150839058834
lambda=  1000.0  no of non-zero in weight vector 21333 error=  0.111753077968
lambda=  10000.0  no of non-zero in weight vector 34 error=  0.248966762935

    \end{Verbatim}

    \hypertarget{feature-collinearity-test-perbutation-test}{%
\subsubsection{feature collinearity test-perbutation
test}\label{feature-collinearity-test-perbutation-test}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{n}{df}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cleaned\PYZus{}amazon\PYZus{}reviews.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{df}\PY{o}{=}\PY{n}{df}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{c+c1}{\PYZsh{}for balanced dataset where n=\PYZsh{}for each class}
        \PY{n}{n}\PY{o}{=}\PY{l+m+mi}{2500}
        \PY{n}{df\PYZus{}positive}\PY{o}{=}\PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{positive}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{n}{n}\PY{p}{)}
        \PY{n}{df\PYZus{}negative}\PY{o}{=}\PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{negative}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{n}{n}\PY{p}{)}
        \PY{n}{df\PYZus{}sample}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{df\PYZus{}positive}\PY{p}{,}\PY{n}{df\PYZus{}negative}\PY{p}{]}\PY{p}{)}
        \PY{n}{df\PYZus{}sample}\PY{o}{=}\PY{n}{df\PYZus{}sample}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{by}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{ascending}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}  \PY{c+c1}{\PYZsh{}dataframe being sorted based by time }
        
        \PY{n}{list\PYZus{}of\PYZus{}sentence}\PY{o}{=}\PY{p}{[}\PY{p}{]} \PY{c+c1}{\PYZsh{} list of sentences }
        \PY{k}{for} \PY{n}{sentence} \PY{o+ow}{in} \PY{n}{df\PYZus{}sample}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CleanText}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{:}
            \PY{n}{list\PYZus{}of\PYZus{}sentence}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{sentence}\PY{p}{)}
        \PY{n}{tfidf\PYZus{}model}\PY{o}{=}\PY{n}{TfidfVectorizer}\PY{p}{(}\PY{n}{ngram\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}    
        \PY{n}{tfidf\PYZus{}review\PYZus{}matrix}\PY{o}{=}\PY{n}{tfidf\PYZus{}model}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{list\PYZus{}of\PYZus{}sentence}\PY{p}{)} \PY{c+c1}{\PYZsh{}training our tf\PYZhy{}idf model}
        
        \PY{n}{X}\PY{o}{=}\PY{n}{StandardScaler}\PY{p}{(}\PY{n}{with\PYZus{}mean}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{tfidf\PYZus{}review\PYZus{}matrix}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} In LR we know for negative class we take \PYZhy{}1 and for positive class +1.}
        \PY{n}{y}\PY{o}{=}\PY{n}{y}\PY{o}{=}\PY{n}{df\PYZus{}sample}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{string}\PY{p}{:} \PY{l+m+mi}{1} \PY{k}{if} \PY{n}{string}\PY{o}{==}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{positive}\PY{l+s+s1}{\PYZsq{}} \PY{k}{else} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}test}\PY{o}{=}\PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{,}\PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)} \PY{c+c1}{\PYZsh{} 70\PYZhy{}30 split without shuffling(used for TBS)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{lr\PYZus{}model}\PY{o}{=}\PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{tscv}\PY{o}{=}\PY{n}{TimeSeriesSplit}\PY{p}{(}\PY{n}{n\PYZus{}splits}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)} \PY{c+c1}{\PYZsh{} timeseries split for using timeseries based cross\PYZhy{}validation}
        \PY{n}{c}\PY{o}{=}\PY{p}{[}\PY{l+m+mf}{0.0001}\PY{p}{,}\PY{l+m+mf}{0.00001}\PY{p}{,}\PY{l+m+mf}{0.000001}\PY{p}{,}\PY{l+m+mf}{0.0000001}\PY{p}{,}\PY{l+m+mf}{0.00000001}\PY{p}{,}\PY{l+m+mf}{0.000000001}\PY{p}{,}\PY{l+m+mf}{0.0000000001}\PY{p}{]} \PY{c+c1}{\PYZsh{}list of c to passed to the LR model}
        \PY{n}{y\PYZus{}train\PYZus{}sample\PYZus{}weight}\PY{o}{=}\PY{n}{compute\PYZus{}sample\PYZus{}weight}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{balanced}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{y}\PY{o}{=}\PY{n}{y\PYZus{}train}\PY{p}{)}
        \PY{n}{gs}\PY{o}{=}\PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{lr\PYZus{}model}\PY{p}{,}\PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{c}\PY{p}{\PYZcb{}}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{n}{tscv}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{)}
        \PY{n}{gs}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{sample\PYZus{}weight}\PY{o}{=}\PY{n}{y\PYZus{}train\PYZus{}sample\PYZus{}weight}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}5}]:} GridSearchCV(cv=<generator object TimeSeriesSplit.split at 0x3ffefeea8308>,
               error\_score='raise',
               estimator=LogisticRegression(C=1.0, class\_weight=None, dual=False, fit\_intercept=True,
                  intercept\_scaling=1, max\_iter=100, multi\_class='ovr', n\_jobs=-1,
                  penalty='l2', random\_state=0, solver='liblinear', tol=0.0001,
                  verbose=0, warm\_start=False),
               fit\_params=None, iid=True, n\_jobs=1,
               param\_grid=\{'C': [0.0001, 1e-05, 1e-06, 1e-07, 1e-08, 1e-09, 1e-10]\},
               pre\_dispatch='2*n\_jobs', refit=True, return\_train\_score=True,
               scoring='accuracy', verbose=0)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{y\PYZus{}predict}\PY{o}{=}\PY{n}{gs}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
        \PY{n}{y\PYZus{}test\PYZus{}sample\PYZus{}weight}\PY{o}{=}\PY{n}{compute\PYZus{}sample\PYZus{}weight}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{balanced}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{y}\PY{o}{=}\PY{n}{y\PYZus{}test}\PY{p}{)}
        \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}predict}\PY{p}{,}\PY{n}{sample\PYZus{}weight}\PY{o}{=}\PY{n}{y\PYZus{}test\PYZus{}sample\PYZus{}weight}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}6}]:} 0.83675213675213689
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{gs}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}7}]:} \{'C': 1e-07\}
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n}{w1}\PY{o}{=}\PY{n}{gs}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{coef\PYZus{}} \PY{c+c1}{\PYZsh{} storing w as w1 before adding noise to the data}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{X\PYZus{}noise}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{+}\PY{n}{csr\PYZus{}matrix}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mf}{0.10}\PY{p}{,}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} addding noise to the data}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{lr\PYZus{}model}\PY{o}{=}\PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{tscv}\PY{o}{=}\PY{n}{TimeSeriesSplit}\PY{p}{(}\PY{n}{n\PYZus{}splits}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)} \PY{c+c1}{\PYZsh{} timeseries split for using timeseries based cross\PYZhy{}validation}
         \PY{n}{c}\PY{o}{=}\PY{p}{[}\PY{l+m+mf}{0.0001}\PY{p}{,}\PY{l+m+mf}{0.00001}\PY{p}{,}\PY{l+m+mf}{0.000001}\PY{p}{,}\PY{l+m+mf}{0.0000001}\PY{p}{,}\PY{l+m+mf}{0.00000001}\PY{p}{,}\PY{l+m+mf}{0.000000001}\PY{p}{,}\PY{l+m+mf}{0.0000000001}\PY{p}{]} \PY{c+c1}{\PYZsh{}list of c to passed to the LR model}
         \PY{n}{y\PYZus{}train\PYZus{}sample\PYZus{}weight}\PY{o}{=}\PY{n}{compute\PYZus{}sample\PYZus{}weight}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{balanced}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{y}\PY{o}{=}\PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{gs}\PY{o}{=}\PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{lr\PYZus{}model}\PY{p}{,}\PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{c}\PY{p}{\PYZcb{}}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{n}{tscv}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{)}
         \PY{n}{gs}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}noise}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{sample\PYZus{}weight}\PY{o}{=}\PY{n}{y\PYZus{}train\PYZus{}sample\PYZus{}weight}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}11}]:} GridSearchCV(cv=<generator object TimeSeriesSplit.split at 0x3fff01cb6eb8>,
                error\_score='raise',
                estimator=LogisticRegression(C=1.0, class\_weight=None, dual=False, fit\_intercept=True,
                   intercept\_scaling=1, max\_iter=100, multi\_class='ovr', n\_jobs=-1,
                   penalty='l2', random\_state=0, solver='liblinear', tol=0.0001,
                   verbose=0, warm\_start=False),
                fit\_params=None, iid=True, n\_jobs=1,
                param\_grid=\{'C': [0.0001, 1e-05, 1e-06, 1e-07, 1e-08, 1e-09, 1e-10]\},
                pre\_dispatch='2*n\_jobs', refit=True, return\_train\_score=True,
                scoring='accuracy', verbose=0)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{n}{y\PYZus{}predict}\PY{o}{=}\PY{n}{gs}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
         \PY{n}{y\PYZus{}test\PYZus{}sample\PYZus{}weight}\PY{o}{=}\PY{n}{compute\PYZus{}sample\PYZus{}weight}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{balanced}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{y}\PY{o}{=}\PY{n}{y\PYZus{}test}\PY{p}{)}
         \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}predict}\PY{p}{,}\PY{n}{sample\PYZus{}weight}\PY{o}{=}\PY{n}{y\PYZus{}test\PYZus{}sample\PYZus{}weight}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}12}]:} 0.83643162393162407
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{gs}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}13}]:} \{'C': 1e-07\}
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{w2}\PY{o}{=}\PY{n}{gs}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{coef\PYZus{}} \PY{c+c1}{\PYZsh{} storing w as w2 before adding noise to the data}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{dist}\PY{o}{=}\PY{n}{DistanceMetric}\PY{o}{.}\PY{n}{get\PYZus{}metric}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{euclidean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{pairwise}\PY{p}{(}\PY{n}{w1}\PY{p}{,}\PY{n}{w2}\PY{p}{)} \PY{c+c1}{\PYZsh{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{dist}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}19}]:} array([[ 0.00011359]])
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{w1}\PY{p}{,}\PY{n}{w2}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}27}]:} array([[ 0.99538119]])
\end{Verbatim}
            
    \hypertarget{result}{%
\subsubsection{Result:}\label{result}}

\begin{verbatim}
As we can see  that cosine_similarity(w1,w2)=0.99538119 which is approx equal to 1 we can say that mullticollineariry is not present in the data and if it is there it is very very small and hence we can use weight vector as feature importance.
\end{verbatim}

    \hypertarget{feature-imporantance}{%
\subsubsection{Feature Imporantance}\label{feature-imporantance}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{c+c1}{\PYZsh{}reading cleaned amazon review csv}
         \PY{n}{df}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cleaned\PYZus{}amazon\PYZus{}reviews.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{df}\PY{o}{=}\PY{n}{df}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}
         \PY{n}{df\PYZus{}sample}\PY{o}{=}\PY{n}{df} \PY{c+c1}{\PYZsh{}here we can modify our sample size}
         \PY{n}{df\PYZus{}sample}\PY{o}{=}\PY{n}{df\PYZus{}sample}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{by}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{ascending}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}  \PY{c+c1}{\PYZsh{} sorting of dataframe for Time Based splitting}
         \PY{c+c1}{\PYZsh{} converting reviews to vector}
         \PY{n}{list\PYZus{}of\PYZus{}sentence}\PY{o}{=}\PY{p}{[}\PY{p}{]} \PY{c+c1}{\PYZsh{}list of sentence to be used for countvectorizer}
         \PY{k}{for} \PY{n}{sentence} \PY{o+ow}{in} \PY{n}{df\PYZus{}sample}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CleanText}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{:}
             \PY{n}{li}\PY{o}{=}\PY{n}{sentence}
             \PY{n}{list\PYZus{}of\PYZus{}sentence}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{li}\PY{p}{)}
         
         \PY{n}{tfidf\PYZus{}model}\PY{o}{=}\PY{n}{TfidfVectorizer}\PY{p}{(}\PY{n}{ngram\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}using count\PYZhy{}BOW and 1 \PYZam{} 2 grams }
         \PY{n}{tfidf\PYZus{}bow\PYZus{}review\PYZus{}matrix}\PY{o}{=}\PY{n}{tfidf\PYZus{}model}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{list\PYZus{}of\PYZus{}sentence}\PY{p}{)} \PY{c+c1}{\PYZsh{}training our model and converting text to vector}
         
         \PY{c+c1}{\PYZsh{} standardization of feature matrix}
         \PY{n}{X}\PY{o}{=}\PY{n}{StandardScaler}\PY{p}{(}\PY{n}{with\PYZus{}mean}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{tfidf\PYZus{}bow\PYZus{}review\PYZus{}matrix}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} In LR we know for negative class we take \PYZhy{}1 and for positive class +1.}
         \PY{n}{y}\PY{o}{=}\PY{n}{y}\PY{o}{=}\PY{n}{df\PYZus{}sample}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{string}\PY{p}{:} \PY{l+m+mi}{1} \PY{k}{if} \PY{n}{string}\PY{o}{==}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{positive}\PY{l+s+s1}{\PYZsq{}} \PY{k}{else} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}test}\PY{o}{=}\PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{,}\PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)} \PY{c+c1}{\PYZsh{} 70\PYZhy{}30 split without shuffling(used for TBS)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}35}]:} (230100, 3178748)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{n}{n}\PY{o}{=}\PY{l+m+mi}{100} \PY{c+c1}{\PYZsh{}top n features }
         \PY{n}{y\PYZus{}train\PYZus{}sample\PYZus{}weight}\PY{o}{=}\PY{n}{compute\PYZus{}sample\PYZus{}weight}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{balanced}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{y}\PY{o}{=}\PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{lr\PYZus{}model}\PY{o}{=}\PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{C}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}07}\PY{p}{,}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{c+c1}{\PYZsh{} training the model for Weight vector}
         \PY{n}{lr\PYZus{}model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{sample\PYZus{}weight}\PY{o}{=}\PY{n}{y\PYZus{}train\PYZus{}sample\PYZus{}weight}\PY{p}{)}
         
         \PY{n}{di\PYZus{}positive}\PY{o}{=}\PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}\PY{c+c1}{\PYZsh{}dict for storing key as +ve w components  and value as index corrosponding to that component}
         \PY{n}{di\PYZus{}negative}\PY{o}{=}\PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}\PY{c+c1}{\PYZsh{}dict for storing key as \PYZhy{}ve w components  and value as index corrosponding to that component}
         
         \PY{n}{li}\PY{o}{=}\PY{n}{lr\PYZus{}model}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{]}
         \PY{n}{feature\PYZus{}name}\PY{o}{=}\PY{n}{tfidf\PYZus{}model}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{} features name }
         \PY{k}{for} \PY{n}{value}\PY{p}{,}\PY{n}{key} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{li}\PY{p}{)}\PY{p}{:}
             \PY{k}{if} \PY{n}{key}\PY{o}{\PYZgt{}}\PY{l+m+mi}{0}\PY{p}{:}
                 \PY{n}{di\PYZus{}positive}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{o}{=}\PY{n}{value}
             \PY{k}{else}\PY{p}{:}
                 \PY{n}{di\PYZus{}negative}\PY{p}{[}\PY{n+nb}{abs}\PY{p}{(}\PY{n}{key}\PY{p}{)}\PY{p}{]}\PY{o}{=}\PY{n}{value}
         \PY{n}{top\PYZus{}positive\PYZus{}features}\PY{o}{=}\PY{p}{[}\PY{p}{]}
         \PY{n}{top\PYZus{}negative\PYZus{}features}\PY{o}{=}\PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{key} \PY{o+ow}{in} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{di\PYZus{}positive}\PY{p}{,}\PY{n}{reverse}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{n}{n}\PY{p}{]}\PY{p}{:} \PY{c+c1}{\PYZsh{}sorted the dict in descending order to get  n imp +ve weights components}
             \PY{n}{index}\PY{o}{=}\PY{n}{di\PYZus{}positive}\PY{p}{[}\PY{n}{key}\PY{p}{]} \PY{c+c1}{\PYZsh{}fetching the}
             \PY{n}{top\PYZus{}positive\PYZus{}features}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{feature\PYZus{}name}\PY{p}{[}\PY{n}{index}\PY{p}{]}\PY{p}{)}
             
         \PY{k}{for} \PY{n}{key} \PY{o+ow}{in} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{di\PYZus{}negative}\PY{p}{,}\PY{n}{reverse}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{n}{n}\PY{p}{]}\PY{p}{:} \PY{c+c1}{\PYZsh{}sorted the dict in descending order to get  n imp \PYZhy{}ve weights components}
             \PY{n}{index}\PY{o}{=}\PY{n}{di\PYZus{}negative}\PY{p}{[}\PY{n}{key}\PY{p}{]} \PY{c+c1}{\PYZsh{}fetching the}
             \PY{n}{top\PYZus{}negative\PYZus{}features}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{feature\PYZus{}name}\PY{p}{[}\PY{n}{index}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{n}{top\PYZus{}positive\PYZus{}features} \PY{c+c1}{\PYZsh{} positive features}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}39}]:} ['great',
          'love',
          'best',
          'delici',
          'good',
          'find',
          'perfect',
          'favorit',
          'make',
          'use',
          'easi',
          'high recommend',
          'excel',
          'nice',
          'keep',
          'enjoy',
          'wonder',
          'this',
          'snack',
          'year',
          'store',
          'littl',
          'alway',
          'add',
          'high',
          'tasti',
          'thank',
          'quick',
          'recommend',
          'also',
          'well',
          'price',
          'without',
          'carri',
          'happi',
          'day',
          'everi',
          'it',
          'tast great',
          'found',
          'these',
          'smooth',
          'ive',
          'amaz',
          'fresh',
          'healthi',
          'morn',
          'famili',
          'fast',
          'tea',
          'glad',
          'satisfi',
          'treat',
          'friend',
          'this best',
          'need',
          'right',
          'this great',
          'addict',
          'bit',
          'time',
          'rich',
          'great tast',
          'mix',
          'hard find',
          'long',
          'great product',
          'cook',
          'yummi',
          'definit',
          'flavor',
          'free',
          'awesom',
          'husband',
          'avail',
          'fantast',
          'work',
          'beat',
          'breakfast',
          'home',
          'great price',
          'meal',
          'hook',
          'sweet',
          'they',
          'kid',
          'help',
          'one best',
          'local',
          'make great',
          'calori',
          'cold',
          'especi',
          'abl',
          'realli good',
          'conveni',
          'sometim',
          'everyon',
          'ice',
          'delight']
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{n}{top\PYZus{}negative\PYZus{}features}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}40}]:} ['disappoint',
          'not',
          'return',
          'wast',
          'wast money',
          'worst',
          'not buy',
          'aw',
          'horribl',
          'would not',
          'terribl',
          'bad',
          'money',
          'threw',
          'not recommend',
          'not worth',
          'refund',
          'stale',
          'disgust',
          'veri disappoint',
          'wont buy',
          'not good',
          'dont wast',
          'thought',
          'descript',
          'not order',
          'not purchas',
          'tast like',
          'away',
          'unfortun',
          'wors',
          'poor',
          'mayb',
          'pictur',
          'never buy',
          'receiv',
          'didnt',
          'nasti',
          'bewar',
          'gross',
          'not even',
          'throw',
          'expir',
          'bland',
          'yuck',
          'tasteless',
          'trash',
          'rip',
          'contact',
          'stuck',
          'mistak',
          'threw away',
          'would',
          'sorri',
          'review',
          'noth like',
          'weak',
          'never order',
          'ined',
          'two star',
          'throw away',
          'bad batch',
          'tast aw',
          'end throw',
          'buyer bewar',
          'mislead',
          'worst tast',
          'tast bad',
          'hope',
          'unpleas',
          'cancel',
          'item',
          'list',
          'label',
          'disappoint product',
          'china',
          'product not',
          'tast horribl',
          'not tast',
          'will not',
          'want like',
          'definit not',
          'tast noth',
          'tast terribl',
          'realli disappoint',
          'this not',
          'read',
          'open',
          'noth',
          'extrem disappoint',
          'guess',
          'wouldnt',
          'date',
          'got bad',
          'high hope',
          'not return',
          'buyer',
          'box',
          'dont buy',
          'this worst']
\end{Verbatim}
            

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
